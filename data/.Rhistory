#Now use the activation partials and alpha to update the weights
for(i in 1:length(weights)){
for(j in 1:length(weights[[i]])){
for(k in 1:length(weights[[i]][[j]])) {
#update the weight by the step size times the partial derivative.  Note this is very similar
#to our node_partial calculation, except that d/dWeight is different than d/dX (we multiply
#by layer value instead of by weight value)
weights[[i]][[j]][k] = weights[[i]][[j]][k] - alpha *
sigmoid_prime(matrix(layers[[i]], nrow=1), as.matrix(weights[[i]][[j]]), bias[[i]][[j]]) *
layers[[i]][k] *
node_partials[[i+1]][[j]]
}
bias[[i]][[j]] = bias[[i]][[j]] - alpha *
sigmoid_prime(matrix(layers[[i]], nrow=1), as.matrix(weights[[i]][[j]]), bias[[i]][[j]]) *
node_partials[[i+1]][[j]]
}
}
print(paste(c("Updated Weights:", weights)))
}
plot(graph_weight)
return(list(weights, bias))
}
results = jnn(X, Y, neurons = neurons, alpha=.1, iterations = 10000)
#' Jeneral Neural Net
#'
#' This function is a custom implementation of a neural network
#' @param X the input matrix
#' @param Y the output input matrix
#' @keywords jeneral neural net network
#' @export
#' @examples
#' jnn(X, Y)
jnn <- function(X, Y, neurons, alpha=0.01,
iterations=1000,
...){
#custom implementation to train a neural network based on X and Y
#weights is a 3 dimensional array
#index 1 is which layer we are in
#index 2 is which neuron in that layer we are looking at
#index 3 is which neuron in the previous layer we are associated with
graph_weight <- vector()
bias = list()
weights = list()
for(i in 1:(length(neurons) + 1)){
weights[[i]] = list()
bias[[i]] = list()
if(i > length(neurons)){
#these are the weights from the last layer to the output
for(j in 1:ncol(Y)){
weights[[i]][[j]] = matrix(rep(0, length(weights[[i-1]])), nrow=length(weights[[i-1]]))
bias[[i]][[j]] = runif(min=-1, max=1, 1)
}
} else {
for(j in 1:neurons[i]){
bias[[i]][[j]] = runif(min=-1, max=1, 1)
if (i > 1) {
weights[[i]][[j]] = matrix(rep(0, length(weights[[i-1]])), nrow=length(weights[[i-1]]))
}
else {
#these are the weights assoociated with the inputs
weights[[i]][[j]] = matrix(rep(0, ncol(X)), nrow=ncol(X))
}
}
}
}
#for sgd, need to set batch size
batch_size = 1
for(iteration in 1:iterations){
if(iteration %% 1000 == 1){
print("Iteration:")
print(iteration)
}
graph_weight <- c(graph_weight, bias[[1]][[1]])
#consume 1 random observation
#runif returns float not int! index = runif(1, min=1, max=nrow(Y))
index = sample(1:nrow(Y), 1)
#print(c("training on: ",index))
x = X[index,]
y = Y[index,]
index
layers = forward_propogate_values(weights, bias, x)
#backpropogate gradient through network
#to backpropogate errors, we use the chain rule.
#to find the derivative of the error with respect to a weight, we first find the partial derivative
#of the error with respect to the activation function that our weight touches, and multiply it by
#the partial derivative of that activation funtion with respect to our weight
#we do this using dynamic programming to build an array of partial derivatives for each
#node value.  To find our weight partials, we will then iterate through that and apply
#take the partial derivative at that node with respect to the weight
node_partials = list()
#the last node partial slot describes the partial derivative of the error function
#with respect to our outpus
node_partials[[length(layers)]] = list()
for(j in 1:length(layers[length(layers)])){
#assuming a loss function of y(1-y^) + (1-y)y^
#dLoss/dy^ = 1-2y.  if y = 0, derivative is 1, else y = 1, derivative is -1
node_partials[[length(layers)]][[j]] = (1-2*y[j])
}
node_partials
i = 2
for(i in (length(layers) - 1):1){
#find the partial derivative of the error function with respect to each node
#Note that we don't actually care about the input nodes, but we calculate them because
#it's easier than special casing it
node_partials[[i]] = list()
j = 1
for(j in 1:length(layers[[i]])){
#node_partials[[i]][[j]] is the partial derivative of the error with respect to
#the value of node j in layer i
#this is equal to the sum of (the derivatives of the error with respect
#to the nodes layer i + 1, times the derivative of those node values with
#respect to node j in layer i)
#the partial derivate of a node n in layer i + 1 with respect to node j in layer i is
#equal to the derivative of node n with respect to the activation function of node n times
#the derivative of the activation function with respect to node j
#if we're using sigmoid as the activation function, and a linear combination of inputs
#as the input to the sigmoid function, this yields sigmoid_prime * w, where w is the weight
#between node j and node n
val = 0
k=1
for(k in 1:length(layers[[i+1]])){
#k is the number of nodes in the next layer (assuming fully connected layers)
val = val + sigmoid_prime(matrix(layers[[i]], nrow=1), as.numeric(weights[[i]][[k]]), bias[[i]][[k]]) *
weights[[i]][[k]][j] *
node_partials[[i+1]][[k]]
}
node_partials[[i]][[j]] = val
}
#step the weights in the direction of the derivative
#calculate the derivative
#modify the weights and bias
}
# print(paste(c("Node Partials:", node_partials)))
# print(paste(c("Weights:", weights)))
# print(paste(c("Bias:", bias)))
# print(paste(c("Layers:", layers)))
#
#Now use the activation partials and alpha to update the weights
for(i in 1:length(weights)){
for(j in 1:length(weights[[i]])){
for(k in 1:length(weights[[i]][[j]])) {
#update the weight by the step size times the partial derivative.  Note this is very similar
#to our node_partial calculation, except that d/dWeight is different than d/dX (we multiply
#by layer value instead of by weight value)
weights[[i]][[j]][k] = weights[[i]][[j]][k] - alpha *
sigmoid_prime(matrix(layers[[i]], nrow=1), as.matrix(weights[[i]][[j]]), bias[[i]][[j]]) *
layers[[i]][k] *
node_partials[[i+1]][[j]]
}
bias[[i]][[j]] = bias[[i]][[j]] - alpha *
sigmoid_prime(matrix(layers[[i]], nrow=1), as.matrix(weights[[i]][[j]]), bias[[i]][[j]]) *
node_partials[[i+1]][[j]]
}
}
#print(paste(c("Updated Weights:", weights)))
}
plot(graph_weight)
return(list(weights, bias))
}
results = jnn(X, Y, neurons = neurons, alpha=.1, iterations = 10000)
#' Jeneral Neural Net
#'
#' This function is a custom implementation of a neural network
#' @param X the input matrix
#' @param Y the output input matrix
#' @keywords jeneral neural net network
#' @export
#' @examples
#' jnn(X, Y)
jnn <- function(X, Y, neurons, alpha=0.01,
iterations=1000,
...){
#custom implementation to train a neural network based on X and Y
#weights is a 3 dimensional array
#index 1 is which layer we are in
#index 2 is which neuron in that layer we are looking at
#index 3 is which neuron in the previous layer we are associated with
graph_weight <- vector()
bias = list()
weights = list()
for(i in 1:(length(neurons) + 1)){
weights[[i]] = list()
bias[[i]] = list()
if(i > length(neurons)){
#these are the weights from the last layer to the output
for(j in 1:ncol(Y)){
weights[[i]][[j]] = matrix(rep(0, length(weights[[i-1]])), nrow=length(weights[[i-1]]))
bias[[i]][[j]] = runif(min=-1, max=1, 1)
}
} else {
for(j in 1:neurons[i]){
bias[[i]][[j]] = runif(min=-1, max=1, 1)
if (i > 1) {
weights[[i]][[j]] = matrix(rep(0, length(weights[[i-1]])), nrow=length(weights[[i-1]]))
}
else {
#these are the weights assoociated with the inputs
weights[[i]][[j]] = matrix(rep(0, ncol(X)), nrow=ncol(X))
}
}
}
}
#for sgd, need to set batch size
batch_size = 1
for(iteration in 1:iterations){
if(iteration %% 1000 == 0){
print("Iteration:")
print(iteration)
}
graph_weight <- c(graph_weight, bias[[1]][[1]])
#consume 1 random observation
#runif returns float not int! index = runif(1, min=1, max=nrow(Y))
index = sample(1:nrow(Y), 1)
#print(c("training on: ",index))
x = X[index,]
y = Y[index,]
index
layers = forward_propogate_values(weights, bias, x)
#backpropogate gradient through network
#to backpropogate errors, we use the chain rule.
#to find the derivative of the error with respect to a weight, we first find the partial derivative
#of the error with respect to the activation function that our weight touches, and multiply it by
#the partial derivative of that activation funtion with respect to our weight
#we do this using dynamic programming to build an array of partial derivatives for each
#node value.  To find our weight partials, we will then iterate through that and apply
#take the partial derivative at that node with respect to the weight
node_partials = list()
#the last node partial slot describes the partial derivative of the error function
#with respect to our outpus
node_partials[[length(layers)]] = list()
for(j in 1:length(layers[length(layers)])){
#assuming a loss function of y(1-y^) + (1-y)y^
#dLoss/dy^ = 1-2y.  if y = 0, derivative is 1, else y = 1, derivative is -1
node_partials[[length(layers)]][[j]] = (1-2*y[j])
}
node_partials
i = 2
for(i in (length(layers) - 1):1){
#find the partial derivative of the error function with respect to each node
#Note that we don't actually care about the input nodes, but we calculate them because
#it's easier than special casing it
node_partials[[i]] = list()
j = 1
for(j in 1:length(layers[[i]])){
#node_partials[[i]][[j]] is the partial derivative of the error with respect to
#the value of node j in layer i
#this is equal to the sum of (the derivatives of the error with respect
#to the nodes layer i + 1, times the derivative of those node values with
#respect to node j in layer i)
#the partial derivate of a node n in layer i + 1 with respect to node j in layer i is
#equal to the derivative of node n with respect to the activation function of node n times
#the derivative of the activation function with respect to node j
#if we're using sigmoid as the activation function, and a linear combination of inputs
#as the input to the sigmoid function, this yields sigmoid_prime * w, where w is the weight
#between node j and node n
val = 0
k=1
for(k in 1:length(layers[[i+1]])){
#k is the number of nodes in the next layer (assuming fully connected layers)
val = val + sigmoid_prime(matrix(layers[[i]], nrow=1), as.numeric(weights[[i]][[k]]), bias[[i]][[k]]) *
weights[[i]][[k]][j] *
node_partials[[i+1]][[k]]
}
node_partials[[i]][[j]] = val
}
#step the weights in the direction of the derivative
#calculate the derivative
#modify the weights and bias
}
# print(paste(c("Node Partials:", node_partials)))
# print(paste(c("Weights:", weights)))
# print(paste(c("Bias:", bias)))
# print(paste(c("Layers:", layers)))
#
#Now use the activation partials and alpha to update the weights
for(i in 1:length(weights)){
for(j in 1:length(weights[[i]])){
for(k in 1:length(weights[[i]][[j]])) {
#update the weight by the step size times the partial derivative.  Note this is very similar
#to our node_partial calculation, except that d/dWeight is different than d/dX (we multiply
#by layer value instead of by weight value)
weights[[i]][[j]][k] = weights[[i]][[j]][k] - alpha *
sigmoid_prime(matrix(layers[[i]], nrow=1), as.matrix(weights[[i]][[j]]), bias[[i]][[j]]) *
layers[[i]][k] *
node_partials[[i+1]][[j]]
}
bias[[i]][[j]] = bias[[i]][[j]] - alpha *
sigmoid_prime(matrix(layers[[i]], nrow=1), as.matrix(weights[[i]][[j]]), bias[[i]][[j]]) *
node_partials[[i+1]][[j]]
}
}
#print(paste(c("Updated Weights:", weights)))
}
plot(graph_weight)
return(list(weights, bias))
}
results = jnn(X, Y, neurons = neurons, alpha=.1, iterations = 10000)
weights = results[[1]]
bias = results[[2]]
weights
bias
forward_propogate_values(weights, bias, X[1,])
forward_propogate_values(weights, bias, X[2,])
forward_propogate_values(weights, bias, X[3,])
#' Jeneral Neural Net
#'
#' This function is a custom implementation of a neural network
#' @param X the input matrix
#' @param Y the output input matrix
#' @keywords jeneral neural net network
#' @export
#' @examples
#' jnn(X, Y)
jnn <- function(X, Y, neurons, alpha=0.01,
iterations=1000,
...){
#custom implementation to train a neural network based on X and Y
#weights is a 3 dimensional array
#index 1 is which layer we are in
#index 2 is which neuron in that layer we are looking at
#index 3 is which neuron in the previous layer we are associated with
graph_weight <- vector()
bias = list()
weights = list()
for(i in 1:(length(neurons) + 1)){
weights[[i]] = list()
bias[[i]] = list()
if(i > length(neurons)){
#these are the weights from the last layer to the output
for(j in 1:ncol(Y)){
weights[[i]][[j]] = matrix(rep(0, length(weights[[i-1]])), nrow=length(weights[[i-1]]))
bias[[i]][[j]] = runif(min=-1, max=1, 1)
}
} else {
for(j in 1:neurons[i]){
bias[[i]][[j]] = runif(min=-1, max=1, 1)
if (i > 1) {
weights[[i]][[j]] = matrix(rep(0, length(weights[[i-1]])), nrow=length(weights[[i-1]]))
}
else {
#these are the weights assoociated with the inputs
weights[[i]][[j]] = matrix(rep(0, ncol(X)), nrow=ncol(X))
}
}
}
}
#for sgd, need to set batch size
batch_size = 1
for(iteration in 1:iterations){
if(iteration %% 1000 == 0){
print("Iteration:")
print(iteration)
}
graph_weight <- c(graph_weight, weights[[1]][[1]])
#consume 1 random observation
#runif returns float not int! index = runif(1, min=1, max=nrow(Y))
index = sample(1:nrow(Y), 1)
#print(c("training on: ",index))
x = X[index,]
y = Y[index,]
index
layers = forward_propogate_values(weights, bias, x)
#backpropogate gradient through network
#to backpropogate errors, we use the chain rule.
#to find the derivative of the error with respect to a weight, we first find the partial derivative
#of the error with respect to the activation function that our weight touches, and multiply it by
#the partial derivative of that activation funtion with respect to our weight
#we do this using dynamic programming to build an array of partial derivatives for each
#node value.  To find our weight partials, we will then iterate through that and apply
#take the partial derivative at that node with respect to the weight
node_partials = list()
#the last node partial slot describes the partial derivative of the error function
#with respect to our outpus
node_partials[[length(layers)]] = list()
for(j in 1:length(layers[length(layers)])){
#assuming a loss function of y(1-y^) + (1-y)y^
#dLoss/dy^ = 1-2y.  if y = 0, derivative is 1, else y = 1, derivative is -1
node_partials[[length(layers)]][[j]] = (1-2*y[j])
}
node_partials
i = 2
for(i in (length(layers) - 1):1){
#find the partial derivative of the error function with respect to each node
#Note that we don't actually care about the input nodes, but we calculate them because
#it's easier than special casing it
node_partials[[i]] = list()
j = 1
for(j in 1:length(layers[[i]])){
#node_partials[[i]][[j]] is the partial derivative of the error with respect to
#the value of node j in layer i
#this is equal to the sum of (the derivatives of the error with respect
#to the nodes layer i + 1, times the derivative of those node values with
#respect to node j in layer i)
#the partial derivate of a node n in layer i + 1 with respect to node j in layer i is
#equal to the derivative of node n with respect to the activation function of node n times
#the derivative of the activation function with respect to node j
#if we're using sigmoid as the activation function, and a linear combination of inputs
#as the input to the sigmoid function, this yields sigmoid_prime * w, where w is the weight
#between node j and node n
val = 0
k=1
for(k in 1:length(layers[[i+1]])){
#k is the number of nodes in the next layer (assuming fully connected layers)
val = val + sigmoid_prime(matrix(layers[[i]], nrow=1), as.numeric(weights[[i]][[k]]), bias[[i]][[k]]) *
weights[[i]][[k]][j] *
node_partials[[i+1]][[k]]
}
node_partials[[i]][[j]] = val
}
#step the weights in the direction of the derivative
#calculate the derivative
#modify the weights and bias
}
# print(paste(c("Node Partials:", node_partials)))
# print(paste(c("Weights:", weights)))
# print(paste(c("Bias:", bias)))
# print(paste(c("Layers:", layers)))
#
#Now use the activation partials and alpha to update the weights
for(i in 1:length(weights)){
for(j in 1:length(weights[[i]])){
for(k in 1:length(weights[[i]][[j]])) {
#update the weight by the step size times the partial derivative.  Note this is very similar
#to our node_partial calculation, except that d/dWeight is different than d/dX (we multiply
#by layer value instead of by weight value)
weights[[i]][[j]][k] = weights[[i]][[j]][k] - alpha *
sigmoid_prime(matrix(layers[[i]], nrow=1), as.matrix(weights[[i]][[j]]), bias[[i]][[j]]) *
layers[[i]][k] *
node_partials[[i+1]][[j]]
}
bias[[i]][[j]] = bias[[i]][[j]] - alpha *
sigmoid_prime(matrix(layers[[i]], nrow=1), as.matrix(weights[[i]][[j]]), bias[[i]][[j]]) *
node_partials[[i+1]][[j]]
}
}
#print(paste(c("Updated Weights:", weights)))
}
plot(graph_weight)
return(list(weights, bias))
}
results = jnn(X, Y, neurons = neurons, alpha=.1, iterations = 10000)
weights = results[[1]]
bias = results[[2]]
weights
bias
forward_propogate_values(weights, bias, X[1,])
forward_propogate_values(weights, bias, X[2,])
forward_propogate_values(weights, bias, X[3,])
neurons=c(2, 2)
results = jnn(X, Y, neurons = neurons, alpha=.1, iterations = 10000)
weights = results[[1]]
bias = results[[2]]
weights
bias
forward_propogate_values(weights, bias, X[1,])
forward_propogate_values(weights, bias, X[2,])
forward_propogate_values(weights, bias, X[3,])
forward_propogate_values(weights, bias, X[1,])
forward_propogate_values(weights, bias, X[2,])
forward_propogate_values(weights, bias, X[3,])
forward_propogate_values(weights, bias, X[1,])
weights
bias
results = jnn(X, Y, neurons = neurons, alpha=1, iterations = 10000)
weights = results[[1]]
bias = results[[2]]
weights
bias
forward_propogate_values(weights, bias, X[1,])
forward_propogate_values(weights, bias, X[2,])
forward_propogate_values(weights, bias, X[3,])
neurons=c(2)
results = jnn(X, Y, neurons = neurons, alpha=0.1, iterations = 10000)
weights = results[[1]]
bias = results[[2]]
weights
bias
forward_propogate_values(weights, bias, X[1,])
forward_propogate_values(weights, bias, X[2,])
forward_propogate_values(weights, bias, X[3,])
results = jnn(X, Y, neurons = neurons, alpha=1, iterations = 10000)
weights = results[[1]]
bias = results[[2]]
weights
bias
forward_propogate_values(weights, bias, X[1,])
forward_propogate_values(weights, bias, X[2,])
forward_propogate_values(weights, bias, X[3,])
neurons=c(1,1)
results = jnn(X, Y, neurons = neurons, alpha=1, iterations = 10000)
weights = results[[1]]
bias = results[[2]]
weights
bias
forward_propogate_values(weights, bias, X[1,])
forward_propogate_values(weights, bias, X[2,])
forward_propogate_values(weights, bias, X[3,])
weights
bias
forward_propogate_values(weights, bias, X[1,])
forward_propogate_values(weights, bias, X[2,])
forward_propogate_values(weights, bias, X[3,])
forward_propogate_values(weights, bias, X[4,])
X
results = jnn(X, Y, neurons = neurons, alpha=1, iterations = 100000)
weights = results[[1]]
bias = results[[2]]
weights
bias
forward_propogate_values(weights, bias, X[1,])
forward_propogate_values(weights, bias, X[2,])
forward_propogate_values(weights, bias, X[3,])
